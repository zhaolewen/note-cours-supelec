\documentclass{article}
\title{Machine Learning}
\maketitle
\pagebreak

\begin{document}
\section{La regression dans tous ses \'etats}
\subsection{Introduction \`a la r\'egression multiple}
La r\'egression muyltiple permet d'\'etudier la liaison entre une variable (\`a expliquer) $Y$ et un ensemble de $p$ variables explicatives $X_1,\ldots,X_p$.

Le mod\`ele de la r\'egression multiple est d\'efinie par
\begin{equation}
Y=\beta_0+\beta_1 X_1 +\cdots+\beta_p X_p+\epsilon
\end{equation}
o\`u les coefficients de r\'egression $\beta_j$ sont des param\`etres fixe mais inconnus, et $\epsilon$ un terme al\'eatoire suivant une $\mathcal{N}(0,\sigma^2)$

\subsubsection{Donn\'ees et mod\`ele statistique}

On dispose de $n$ observations des variables $X_1,\ldots,X_p$
Soit 
\begin{equation}
X=
\begin{split}
y_1 & x_{11} & \ldots & x_{p1} \\
\ldots & \ldots & \ldots & \ldots \\
y_m & x_{1m} & \ldots & x_{pm}
\end{split}
\end{equation}
(Tableau d'observations)

La mod\`ele statistique est d\'efinie comme 

Pour chaque individu $t$, on consid\`ere que la valeur $y_i$ prise par $Y$ est une r\'ealisation d'une variable al\'eatoire $Y_i$ d\'efinie par 
\begin{equation}
Y_i=\beta_0+\beta_1 X_{1i}+\ldots+\beta_p X_{pi}+\epsilon_i
\end{equation}
o\`u $\sigma_i$ est un terme al\'eatoire suivant une $\mathcal{N}(0,\sigma^2)$. Il faut supposer de plus que les $\epsilon_1,\ldots,\epsilon_m$ sont ind\'ependantes les un les autres.

Sur l'exemple des automobiles, on consid\`ere que le prix $Y_i$ de la voiture $i$ suit une loi normale de moyenne
\begin{equation}
\mu_i=\beta_0+\beta_1 PUISSANCE_i,\ldots,\beta_p LARGEUR_i+\epsilon.
\end{equation} 

\subsubsection{Estimations des param\`etres du mod\`ele}
\`A l'aide des $n$ observations des variables $Y,X_1,\ldots,X_p$, nous allons chercher \`a estimer les param\`etres $\beta_0,\ldots,\beta_p$ du mod\`le.

On cherche $\hat{\beta}=(\hat{\beta}_0,\ldots,\hat{\beta}_p)$ tel que 
\begin{equation}
\hat{\beta}=argmin ||y-X\beta||_2^2=argmin (y-X\beta)^T(y-X\beta)
\end{equation}

G\'eom\'etriquement, cas o\`u $p=1$
\begin{equation}
\hat{y}=\hat{\beta}_0+\hat{\beta}_1 X_1
\end{equation}
On cherche $\hat{\beta}_0$ et $\hat{\beta}_1$ tel que $\sum \epsilon_i^2$ soit minimal.

\begin{equation}
\frac{\partial L}{\partial \beta}=-X^Ty-X^Ty+2X^TX\hat{\beta}=0
\end{equation}
Ainsi
\begin{equation}
\hat{\beta}=(X^TX)^{-1}X^Ty
\end{equation}

remarque 1: Notons que $\hat{\beta}$ est de la forme $Hy$

remarque 2:
\begin{equation}
\hat{\beta}=(X^TX)^{-1}X^Ty=(X^TX)(X^TX)^{-2}X^Ty=X^T\hat{\alpha}
\end{equation}
Cela signifie que $\hat{\beta}$ s'exprime comme la combinaison lin\'eaire des individus
\begin{equation}
\hat{\beta}=\sum_{i=1}^n \hat{\alpha}_ix_i
\end{equation}

Remarque 3
Posons 
\begin{equation}
X=V\Sigma U^T
\end{equation}
(D\'ecomposition en valeurs singuliers)
\begin{equation}
V^TV=I
\end{equation}
et $\Sigma$ est diagonale et $U^TU=I$
Posons 
\begin{equation}
\Lambda=\Sigma^T\Sigma
\end{equation}
matrice diagonale des valeurs de $X^TX$. Ainsi
\begin{equation}
\begin{split}
\hat{\beta}^{OLS}&=(X^TX)^{-1}X^Ty=(U\Sigma V^TV\Sigma U^T)^-1U\Sigma V^T y\\
&=U\Sigma^{-1}\Sigma^{-1}U^TU\Sigma V^Ty\\
&=U\Lambda^{-1}\Sigma V^Ty\\
&=\sum_{j=1}^{P}\frac{v_j^Ty}{\sqrt{\lambda_j}}\mu_j
\end{split}
\end{equation}

Interpr\'etation g\'eom\'etrique de la r\'egression multiple...

Pour un nouvel individu $x$
\begin{equation}
\hat{y}(x)=x^t\hat{\beta}
\end{equation}

Remarque 4

En termes de pr\'ediction, on peut aussi obtenir des expressions duales
\begin{equation}
\hat{y}=x^T\hat{\beta}=\sum_{j=1}^n\alpha_jx^T_jx
\end{equation}
Cette expression duale a la particularit\'e de ne d\'ependre que des produits scalaires entre observations.  

\subsubsection{Qualit\'e des estimations}
On souhaite \'evaluer la pr\'ecision des estimations
\begin{equation}
\begin{split}
\mathbb{E}[\hat{\beta}]&=\mathbb{E}[(X^TX)^{-1}X^TY]=(X^TX)^{-1}X^T\mathbb{E}[Y]\\
&=(X^TX)^{-1}X^T\mathbb{E}[X\beta+\epsilon]\\
&=(X^TX)^{-1}(X^TX)\beta\\
&=\beta
\end{split}
\end{equation}
\begin{equation}
\begin{split}
Var(\hat{\beta})&=var((X^TX)^{-1}X^TY)\\
&=(X^TX)^{-1}X^{T} var(Y) X(X^TX)^{-1}\\
&=\sigma^2 (X^TX)^{-1}
\end{split}
\end{equation}
o\`u
\begin{equation}
var(Y)=\sigma^2I
\end{equation}

Le MSE(Mean Square Error) d'un estimateur $\hat{\beta}$ d'un vecteur $\beta$ est d\'efini
\begin{equation}
\begin{split}
MSE(\hat{\beta})&=\mathbb{E}[tr(\hat{\beta}-\beta)(\hat{\beta}-\beta)^T]\\
&=\mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]\\
&=\mathbb{E}[||\hat{\beta}-\beta||_2^2]\\
&=[\mathbb{E}[\hat{\beta}-\beta]^T][\mathbb{E}[\hat{\beta}]-\beta]+\mathbb{E}[(\hat{\beta}-\methbb{E}[\hat{\beta}])(\hat{\beta}-\mathbb{E}[\hat{\beta}])]\\
&=biais^2(\hat{\beta})+ tr(var(\hat{\beta}))\\
&=\sigma^2tr(X^TX)^{-1}\\
&=\sigma^2\sum_{j=1}^{P}\frac{1}{\lambda_j}
\end{split}
\end{equation}

Si les donn\'ees sont mal-conditionn\'ees, alors
$\Rightarrow$ petit valeurs de $X^TX$
$\Rightarrow$ instabilit\'e des coefficients de regression
$\Rightarrow$ Explosion du MSE
$\Rightarrow$ \'ecart entre $\beta$ et $\hat{\beta}$


Illustration de la multi-lin\'earit\'e

Consid\'erons le mod\`ele suivant
\begin{equation}
Y=\beta_1X_1+\beta_2X_2+\Sigma
\end{equation}
On suppose que les donn\'ees sont standardis\'ees
\begin{equation}
cor(X_1,X_2)=r_{12}, cor(X_j,Y)=r_{jy}
\end{equation}

L'estimation des moindres carr\'ees 
\begin{equation}
\hat{\beta}=(\hat{\beta}_1,\hat{\beta}_2)
\end{equation}
est donn\'e par
\begin{equation}
(X^TX)\hat{\beta}=X^TY
\end{equation}
Comme les donn\'ees sont standardis\'ees
\begin{equation}
\left(\begin{array}{cc}
1 & r_{12} \\
r_{21} & 1
\end{array}\right)
\left(\begin{array}{cc}
\hat{\beta}_1\\
\hat{\beta}_2
\end{array}\right)\sim
\left(\begin{array}{c}
r_{1y}\\
r_{2y}
\end{array}\right)
\end{equation}
L'inverse de $X^TX$ est donn\'ee par 
\begin{equation}
C=(X^TX)^{-1}=\frac{1}{1-r_{12}^2}\left(\begin{array}{cc}
1 & -r_{12}\\
-r_{12} & 1
\end{array}\right)
\end{equation}

On rppelle que
\begin{equation}
var(\hat{\beta})=\sigma^2(X^TX)^{-1}
\end{equation}
donc
\begin{equation}
var(\hat{\beta}_j)=\sigma^2C_jj
\end{equation}

Alors, une forte corr\'elation entre $X_1$ et $X_2$ est indiqu\'ee par $|r_{12}|\to 1$. Ceci implique que $var(\hat{\beta}_j)\to +\infty$.
Ainsi, MSE qui explose.

De mani\'ere g\'en\'erale, on peut montrer que dans le cas de $p$ variables explicatives, des elements diagonaux de $C=(X^TX)^{-1}$ sont \'egaux \`a 
\begin{equation}
C_jj=\frac{1}{1-R_j^2}
\end{equation}
o\`u $R_j^2$ est le coefficient de determination entre $X_j$ et les $p$ autres variables. S'il existe une forte multi-colin\'earit\'e entre $X_j$ et les  $(p-1)$ autres variables. $R_j^2\to 1$

Exemple II

(Voir photos)

\subsection{Facteur de shrinkage}
On rappelle que 
\begin{equation}
\hat{\beta}^{OLS}=(X^TX)^{-1}X^Ty=\sum_{j=1}^{P}\frac{v_j^Ty}{\sqrt{\lambda_j}}\mu_j\sum_{j=1}^{P}\z_j
\end{equation}

Dans sa forme g\'en\'erale, on f\'efinit un estimateur g\'en\'erale de $\beta$ par
\begin{equation}
\hat{\beta}^{shi}=\sum_{j=1}^{P}f(\lambda_j)z_j
\end{equation}

$f(\lambda_j)$ est un facteur de shrinkage qui va jouer sur le MSE. Ici, on se concentre sur les facteurs de shrinkage ind\'ependant de $y$. Malheureusement, on a 
\begin{equation}
\hat{\beta}^{shi}=U\Sigma^{-1}DV^Ty=H^{shi}y
\end{equation}
avec
\begin{equation}
D=diag(f(\lambda_1),\ldots,f(\lambda_p))
\end{equation}
\'Etudions l'influence de 
(Photos)

\begin{equation}
\begin{split}
\hat{\beta}=&argmin_{\beta\in\mathbb{R}^p}\{||y-X\beta||+\lambda||\beta||_2^2\}\\
=&(X^TX+\lambdaI_p)^{-1}X^Ty\\
=&\sum_{j=1}^p\frac{\lambda_j}{\lambda_j+\lambda}\frac{v^T_jy}{\sqrt{\lambda_j}}\\
=&\sum_{j=1}^p f(\lambda_j)z_j
\end{split}
\end{equation}

\begin{equation}
\hat{y}_\lambda=X\hat{\beta}_\lambda^{RR}=V\SigmaU^TU(\Lambda+\lambda I)^{-1}\Sigma\Lambda y=V\Sigma(\Lambda+\lambda I)^{-1}\Sigma V^T y
\end{equation}

\subsubsection{Choix de mod\`ele (\lambda) + ridge path validation crois\'ee}
\begin{itemize}
        \item Ensemble d'apprentissage permet de construire le mod\`ele
        \item Ensemble de test permet d'\'evaluer la qualit\'e du mod\`ele 
\end{itemize}

N\'ecessite de d\'eterminer $\lambda$ sur la base d'un crit\'ere objectif

Un mod\`ele devrait predire efficacement des individus qui n'ont pas servi \`a sa construction

Pour \'evaluer la qualit\'e du mod\`ele, on utilise des strat\'egies de validation crois\'ee.

K-fold cross validation
\begin{enumerate}
\item Partition du jeu de donn\'ees en $K$ parties de taille \'egale $T_1,\ldots,T_K$
\item Pour chaque $k=1,\ldots,K$ construire $\hat{\beta}_\lambda^{-k}$ sans $T_k$
\item $\hat{y}_\lambda^{-k}=T_k\hat{\beta}_\lambda^{-k}$
\item Calcul d'erreurs en test
\begin{equation}
CV_{erreur}(\lambda,k)=\frac{1}{n_k}\sum(y_k-\hat{y}_\lambda^{-k})^2
\end{equation}
Ainsi, Taux d'erreur global($\lambda$)
\begin{equation}
=\frac{1}{K}\sum_{k=1}^KCV_{erreur}(\lambda,k)
\end{equation}
Ainsi,
\begin{equation}
\lambda^*=argmin(Taux d'erreur global(\lambda))
\end{equation}
\end{enumerate}

On appelle que 
\begin{equation}
\hat{\beta}^{RR}=argmin\{||y-x||^2_2+\lambda||\beta||_2^2\}=(X^TX+\lambda I)^{-1}X^Ty
\end{equation}

Proposition
$\forall \lambda>0$ on a 
\begin{equation}
(X^TX+\lambda I_p)^{-1}X^T=X^T(XX^T+\lambda I_\lambda)^{-1}
\end{equation}

Preuve
\begin{equation}
(X^TX+\lambda I_p)^{-1}X^T(XX^T+\lambda I_n)=X^T
\end{equation}
\begin{equation}
(X^TX+\lambdaI_p)^{-1}(X^TX+\lambdaI_p)X^T=X^T
\end{equation}
Il vient de cette proposition une formulation duale pour $\hat{\beta}^{RR}$
\begin{equation}
\begin{split}
\hat{\beta}^{RR}=&X^T(XX^T+\lambda I_n)^{-1}y\\
=&X^T\hat{\alpha}^{RR}
\end{split}
\end{equation}

Remarque: $\hat{\beta}^{RR}$ s'exprime comme une combinaison lin\'eaire des observations.

En termes de pr\'ediction
\begin{equation}
\hat{y}_{\lambda}=X\hat{\beta}_\lambda^{RR}=XX^T\hat{\alpha}^{RR}
\end{equation}
\begin{equation}
h(x)=\sum_{i=1}^n \alpha_i x_i^Tx
\end{equation}

Conclusion

On remarque que $\hat{\alpha}$ et $\hat{y}$ ne s'expriment qu'au
(Photo)

\section{M\'ethode \`a noyaux}
Introduction Supposons que l'on recherche \`a r\'esoudre un probl\`eme de r\'egression non-lin\'eaire.

Il est tout \`a fait possible d'utiliser une fonction de redescription $\Phi:\mathbb{R}^p\to\mathcal{F}$(espace de description).
La solution du probl\`eme de r\'egression lin\'eaire dans l'espace de redescription est de la forme 
\begin{equation}
\begin{split}
h(x)=&<\beta,\Phi(x)>\\
=&\sum_{j=1}^n\beta_j\Phi_j(x)+\beta_0
\end{split}
\end{equation}
avec $Phi(x)=\{\Phi_1(x),\Phi_2(x),\ldots\}$

En r\'esolvant comme pr\'ec\'edemment dans l'espace de redescription, devient la repr\'esentation duale.
\begin{equation}
h(x)=\sum_{i=1}^n\Phi(x_i)^T\Phi(x)
\end{equation}

En rapprochant $h(x)=\beta^T\Phi(x)$ et $h(x)=\sum_{i=1}^n\alpha_i\Phi(n_i)^T\Phi(x)$
devient 
\begin{equation}
\beta^T=\sum_{i=1}^n\alpha_i\Phi(x_i)^T
\end{equation}

Commentaire 1: Comment trouver une fonction de redescription ad\'equate?

Commentaire 2: Co\^{u}t calculatoire des produits scalaires dans un espace $\mathcal{F}$ dont la dimension peut \^etre tr\`es grande.

Plut\^ot que de repr\'esenter les observations par un tableau individus $x$ variables, on peut les repr\'esenter par une matrice de similarit\'e de dimension $n\times n$
\begin{equation}
[K]_y=k(x_i,x_j)
\end{equation}
\begin{equation}
[X]\sim [K]
\end{equation}
\begin{equation}
(x_i)_{n\times p}\sim (k(x_i,x_n))_{n\times n}
\end{equation}

Remarque: Mesure de similarit\'e quelle que soit la nature et la complexit\'e des objets.(images, graphs, s\'equences ADN,...)
Remarque 2: La matrice $K$ est toujours de dimension $n\times n$ \forall la taille de l'objet.

Dans ce cours nous nous restrienons \`aune classe particuli\`res de fonction $k$.

D\'efinition Fonction semi definie positive

Une fonction $k$ semi definie positive sur l'ensemble $X$ est une fonction $k:X,X\to \mathcal{R}$ symm\'etrique 
\begin{equation}
\forall (x,x')\in X, k(x,x')=k(x',x)
\end{equation}
et quisatisfait pour 
(Photo)

et $(a_1,\ldots,\a_n)\in\mathcal{R}^n$
\begin{equation}
\sum_{i,j=1}^na_ia_jk(x_i,k_j)>=0
\end{equation}
En d'autres termes $K=(k(x_i,x_j))$ est semi definie positive.

Exemple 1: Le plus simple des noyaux semi definie positif

Soit $X\in\mathbb{R}^P$ et la fonction $k:\mathcal{X}^2\to\mathbb{R}$ definie par
\begin{equation}
\forall (x,x')\in (\mathbb{R}^p)^2, k(x,x')=<x,x'>_{\mathbb{R}^p}
\end{equation}
Sym\'etrie
\begin{equation}
<x,x'>_{\mathbb{R}^p}=<x',x>_{\mathbb{R}^p}
\end{equation}

Th\'eor\`eme de Moose-Aroszajin, 1950

(Photo)

Definition: Soit $\mathcal{H}$

(Photo)

2. La fonction $k$ est une fonction noyau reproduisante 
\begin{equation}
\forall f\in \mathcal{H} on a <f,k(n,.)>=f(x)
\end{equation}

Le produit scalaire est alors d\'efini comme suit:

Soit $f$ et $g\in\mathcal{H}$ d\'efinies par
\begin{equation}
f(x)=\sum_{i=1}^n\alpha_ik(x_i,x)
\end{equation}
et
\begin{equation}
g(x)=\sum_{i=1}^n\beta_jk(x_j,x)
\end{equation}
alors
\begin{equation}
\begin{split}
<f,g>=&\sum_{j=1}^n\sum_{i=1}^n\alpha_i\beta_jk(x_i,x_j)\\
=&\sum_{i=1}^n\alpha_ig(x_i)\\
=&\sum_{j=1}^n\beta_jf(x_j)
\end{split}
\end{equation}

Representation theorem (Kimeldoif & Wabhia 1970)

Soit $\mathcal{H}$ un RKHS associ\'e \`a un noyau $k$.
Soit $\Omega [0,+\infty\[\to\mathbb{R} $ une fonction strictement croissante
Soit $\mathcal{X}$ un emsemble et $l(\mathcal{X}\times\mathbb{R})\to\mathbb{R}$ une fonction de co\^ut.

Alors toute solution du probl\`ele d'optimisation suivant 
\begin{equation}
\min_fJ_f=l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))+\lambda\Omega(||f||_\mathcal{H})
\end{equation}
admet une repr\'esentation de la forme $f^*=\sum_{i=1}^n\alpha_ik(x_i,.)$

Remarque 1
\begin{equation}
l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))+\lambda\Omega(||f||_\mathcal{H})
\end{equation}
conduit \`a la regression rigide.

Remarque 2:
Quelque soit la dimension de l'espace $\mathcal{H}$, la solution \'evolue dans $span\{k(x_i,.),1<=i<=n\}$
de dimension $n$ connue, bien que $H$ puisse \^etre de dimension imporeante.
$\Rightarrow$ Developpement d'algo thno efficace.

\textbf{Preuve du th\'eoreme du representant}
Supposons $f\in\mathcal{H}$ project\'ee sur $span{k(x_i,.),1<=i<=n}$
Soit $f_s$(la composante dans l'espace engendr\'e) et $f_\perp$ la composante orthogonale
\begin{equation}
f=f_s+f_\perp
\end{equation}
Ainsi
\begin{equation}
||f||_\mathcal{H}^2=||f_s||^2_\mathcal{H}+||f_\perp||__\mathcal{H}^2>=||f||_\mathcal{H}^2
\end{equation}
Puisque $\Omega$ est strictement croissante, on a 
\begin{equation}
\Omega(||f||_\methcal{H}^2)>\Omega(||f_s||_\mathcal{H}^2)
\end{equation}
Cela signifie que $\Omega$ est minimis\'ee pour des fonctions $f\in span\{k(x_i,.),1<=i<=n\}$

Par ailleurs, des propri\'et\'es reproduisantes de $k$ on obtient
\begin{equation}
f(x_i)=<f,k(x_i,.)>=<f_s,k(x_i,.)>_\mathcal{H}+<f_\perp,k(x_i,.)>_\mathcal{H}=<f_s,k(x_i,.)>=f_s(x_i)
\end{equation}
Par cons\'equent,
\begin{equation}
l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))=((x_1,y_1,f_s(x_1)),\ldots,(x_n,y_n,f_s(x_n)))
\end{equation}
Donc $l$ ne d\'epend que des composantes de $f\in span\{k(x_i,.),1<=i<=n\}$ et $\Omega$ est minimis\'ee que si $f\in$ cet espace

Conclusion:
$J(f)$ est minimis\'ee que si $f\in span\{k(x_i,.),1<=i<=n\}$ et ;a solution est fonc de la forme
\begin{equation}
f^*=\sum_{i=1}^n\alpha_ik(x_i,.)
\end{equation}

Kernel Rigide Regression

Un point d'une fonctionnelle

Soit $\mathcal{H}$ un RKHS de noyau $k$. On consid\`ere le probl\`eme suivant
\begin{equation}
f^*=argmin_{f\mathcal{H}}\{\frac{1}{2}\sum_{i=1}^n (y_i-f(x_i))^2+\frac{\lambda}{2}||f||_\mathcal{H}^2\}
\end{equation}
$=$ termed'attache aux donn\'ees $+$ terme de r\'egularisation.

Par le th\'eor\`eme de repr\'esentant, on sait que 
\begin{equation}
f^*\sum_{j=1}^n\alpha_j k(x_j,.)
\end{equation}

(Photo)

Exercise: Montrer que

(Photo)

Kernel Ridige Regression


\end{document}