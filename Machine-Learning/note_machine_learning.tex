\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}

\title{Machine Learning}

\begin{document}
\maketitle
\pagebreak
\section{La regression dans tous ses \'etats}
\subsection{Introduction \`a la r\'egression multiple}
La r\'egression muyltiple permet d'\'etudier la liaison entre une variable (\`a expliquer) $Y$ et un ensemble de $p$ variables explicatives $X_1,\ldots,X_p$.

Le mod\`ele de la r\'egression multiple est d\'efinie par
\begin{equation}
Y=\beta_0+\beta_1 X_1 +\cdots+\beta_p X_p+\epsilon
\end{equation}
o\`u les coefficients de r\'egression $\beta_j$ sont des param\`etres fixe mais inconnus, et $\epsilon$ un terme al\'eatoire suivant une $\mathcal{N}(0,\sigma^2)$

\subsubsection{Donn\'ees et mod\`ele statistique}

On dispose de $n$ observations des variables $X_1,\ldots,X_p$
Soit 
\begin{equation}
X=\left[
\begin{array}{cccc}
y_1 & x_{11} & \ldots & x_{p1} \\
\ldots & \ldots & \ldots & \ldots \\
y_m & x_{1m} & \ldots & x_{pm}
\end{array}\right]
\end{equation}
(Tableau d'observations)

La mod\`ele statistique est d\'efinie comme 

Pour chaque individu $t$, on consid\`ere que la valeur $y_i$ prise par $Y$ est une r\'ealisation d'une variable al\'eatoire $Y_i$ d\'efinie par 
\begin{equation}
Y_i=\beta_0+\beta_1 X_{1i}+\ldots+\beta_p X_{pi}+\epsilon_i
\end{equation}
o\`u $\sigma_i$ est un terme al\'eatoire suivant une $\mathcal{N}(0,\sigma^2)$. Il faut supposer de plus que les $\epsilon_1,\ldots,\epsilon_m$ sont ind\'ependantes les un les autres.

Sur l'exemple des automobiles, on consid\`ere que le prix $Y_i$ de la voiture $i$ suit une loi normale de moyenne
\begin{equation}
\mu_i=\beta_0+\beta_1 PUISSANCE_i,\ldots,\beta_p LARGEUR_i+\epsilon.
\end{equation} 

\subsubsection{Estimations des param\`etres du mod\`ele}
\`A l'aide des $n$ observations des variables $Y,X_1,\ldots,X_p$, nous allons chercher \`a estimer les param\`etres $\beta_0,\ldots,\beta_p$ du mod\`le.

On cherche $\hat{\beta}=(\hat{\beta}_0,\ldots,\hat{\beta}_p)$ tel que 
\begin{equation}
\hat{\beta}=argmin ||y-X\beta||_2^2=argmin (y-X\beta)^T(y-X\beta)
\end{equation}

G\'eom\'etriquement, cas o\`u $p=1$
\begin{equation}
\hat{y}=\hat{\beta}_0+\hat{\beta}_1 X_1
\end{equation}
On cherche $\hat{\beta}_0$ et $\hat{\beta}_1$ tel que $\sum \epsilon_i^2$ soit minimal.

\begin{equation}
\frac{\partial L}{\partial \beta}=-X^Ty-X^Ty+2X^TX\hat{\beta}=0
\end{equation}
Ainsi
\begin{equation}
\hat{\beta}=(X^TX)^{-1}X^Ty
\end{equation}

remarque 1: Notons que $\hat{\beta}$ est de la forme $Hy$

remarque 2:
\begin{equation}
\hat{\beta}=(X^TX)^{-1}X^Ty=(X^TX)(X^TX)^{-2}X^Ty=X^T\hat{\alpha}
\end{equation}
Cela signifie que $\hat{\beta}$ s'exprime comme la combinaison lin\'eaire des individus
\begin{equation}
\hat{\beta}=\sum_{i=1}^n \hat{\alpha}_ix_i
\end{equation}

Remarque 3
Posons 
\begin{equation}
X=V\Sigma U^T
\end{equation}
(D\'ecomposition en valeurs singuliers)
\begin{equation}
V^TV=I
\end{equation}
et $\Sigma$ est diagonale et $U^TU=I$
Posons 
\begin{equation}
\Lambda=\Sigma^T\Sigma
\end{equation}
matrice diagonale des valeurs de $X^TX$. Ainsi
\begin{equation}
\begin{split}
\hat{\beta}^{OLS}&=(X^TX)^{-1}X^Ty=(U\Sigma V^TV\Sigma U^T)^-1U\Sigma V^T y\\
&=U\Sigma^{-1}\Sigma^{-1}U^TU\Sigma V^Ty\\
&=U\Lambda^{-1}\Sigma V^Ty\\
&=\sum_{j=1}^{P}\frac{v_j^Ty}{\sqrt{\lambda_j}}\mu_j
\end{split}
\end{equation}

Interpr\'etation g\'eom\'etrique de la r\'egression multiple...

Pour un nouvel individu $x$
\begin{equation}
\hat{y}(x)=x^t\hat{\beta}
\end{equation}

Remarque 4

En termes de pr\'ediction, on peut aussi obtenir des expressions duales
\begin{equation}
\hat{y}=x^T\hat{\beta}=\sum_{j=1}^n\alpha_jx^T_jx
\end{equation}
Cette expression duale a la particularit\'e de ne d\'ependre que des produits scalaires entre observations.  

\subsubsection{Qualit\'e des estimations}
On souhaite \'evaluer la pr\'ecision des estimations
\begin{equation}
\begin{split}
\mathbb{E}[\hat{\beta}]&=\mathbb{E}[(X^TX)^{-1}X^TY]=(X^TX)^{-1}X^T\mathbb{E}[Y]\\
&=(X^TX)^{-1}X^T\mathbb{E}[X\beta+\epsilon]\\
&=(X^TX)^{-1}(X^TX)\beta\\
&=\beta
\end{split}
\end{equation}
\begin{equation}
\begin{split}
Var(\hat{\beta})&=var((X^TX)^{-1}X^TY)\\
&=(X^TX)^{-1}X^{T} var(Y) X(X^TX)^{-1}\\
&=\sigma^2 (X^TX)^{-1}
\end{split}
\end{equation}
o\`u
\begin{equation}
var(Y)=\sigma^2I
\end{equation}

Le MSE(Mean Square Error) d'un estimateur $\hat{\beta}$ d'un vecteur $\beta$ est d\'efini
\begin{equation}
\begin{split}
MSE(\hat{\beta})&=\mathbb{E}[tr(\hat{\beta}-\beta)(\hat{\beta}-\beta)^T]\\
&=\mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]\\
&=\mathbb{E}[||\hat{\beta}-\beta||_2^2]\\
&=[\mathbb{E}[\hat{\beta}-\beta]^T][\mathbb{E}[\hat{\beta}]-\beta]+\mathbb{E}[(\hat{\beta}-\mathbb{E}[\hat{\beta}])(\hat{\beta}-\mathbb{E}[\hat{\beta}])]\\
&=biais^2(\hat{\beta})+ tr(var(\hat{\beta}))\\
&=\sigma^2tr(X^TX)^{-1}\\
&=\sigma^2\sum_{j=1}^{P}\frac{1}{\lambda_j}
\end{split}
\end{equation}

Si les donn\'ees sont mal-conditionn\'ees, alors
$\Rightarrow$ petit valeurs de $X^TX$
$\Rightarrow$ instabilit\'e des coefficients de regression
$\Rightarrow$ Explosion du MSE
$\Rightarrow$ \'ecart entre $\beta$ et $\hat{\beta}$


Illustration de la multi-lin\'earit\'e

Consid\'erons le mod\`ele suivant
\begin{equation}
Y=\beta_1X_1+\beta_2X_2+\Sigma
\end{equation}
On suppose que les donn\'ees sont standardis\'ees
\begin{equation}
cor(X_1,X_2)=r_{12}, cor(X_j,Y)=r_{jy}
\end{equation}

L'estimation des moindres carr\'ees 
\begin{equation}
\hat{\beta}=(\hat{\beta}_1,\hat{\beta}_2)
\end{equation}
est donn\'e par
\begin{equation}
(X^TX)\hat{\beta}=X^TY
\end{equation}
Comme les donn\'ees sont standardis\'ees
\begin{equation}
\left(\begin{array}{cc}
1 & r_{12} \\
r_{21} & 1
\end{array}\right)
\left(\begin{array}{cc}
\hat{\beta}_1\\
\hat{\beta}_2
\end{array}\right)\sim
\left(\begin{array}{c}
r_{1y}\\
r_{2y}
\end{array}\right)
\end{equation}
L'inverse de $X^TX$ est donn\'ee par 
\begin{equation}
C=(X^TX)^{-1}=\frac{1}{1-r_{12}^2}\left(\begin{array}{cc}
1 & -r_{12}\\
-r_{12} & 1
\end{array}\right)
\end{equation}

On rappelle que
\begin{equation}
var(\hat{\beta})=\sigma^2(X^TX)^{-1}
\end{equation}
donc
\begin{equation}
var(\hat{\beta}_j)=\sigma^2C_j
\end{equation}

Alors, une forte corr\'elation entre $X_1$ et $X_2$ est indiqu\'ee par $|r_{12}|\to 1$. Ceci implique que $var(\hat{\beta}_j)\to +\infty$.
Ainsi, MSE qui explose.

De mani\'ere g\'en\'erale, on peut montrer que dans le cas de $p$ variables explicatives, des elements diagonaux de $C=(X^TX)^{-1}$ sont \'egaux \`a 
\begin{equation}
C_j=\frac{1}{1-R_j^2}
\end{equation}
o\`u $R_j^2$ est le coefficient de determination entre $X_j$ et les $p$ autres variables. S'il existe une forte multi-colin\'earit\'e entre $X_j$ et les  $(p-1)$ autres variables. $R_j^2\to 1$

Exemple II

(Voir photos)

\subsection{Facteur de shrinkage}
On rappelle que 
\begin{equation}
\hat{\beta}^{OLS}=(X^TX)^{-1}X^Ty=\sum_{j=1}^{P}\frac{v_j^Ty}{\sqrt{\lambda_j}}\mu_j\sum_{j=1}^{P}z_j
\end{equation}

Dans sa forme g\'en\'erale, on f\'efinit un estimateur g\'en\'erale de $\beta$ par
\begin{equation}
\hat{\beta}^{shi}=\sum_{j=1}^{P}f(\lambda_j)z_j
\end{equation}

$f(\lambda_j)$ est un facteur de shrinkage qui va jouer sur le MSE. Ici, on se concentre sur les facteurs de shrinkage ind\'ependant de $y$. Malheureusement, on a 
\begin{equation}
\hat{\beta}^{shi}=U\Sigma^{-1}DV^Ty=H^{shi}y
\end{equation}
avec
\begin{equation}
D=diag(f(\lambda_1),\ldots,f(\lambda_p))
\end{equation}
\'Etudions l'influence de 
(Photos)

\begin{equation}
\begin{split}
\hat{\beta}=&argmin_{\beta\in\mathbb{R}^p}\{||y-X\beta||+\lambda||\beta||_2^2\}\\
=&(X^TX+\lambda I_p)^{-1}X^Ty\\
=&\sum_{j=1}^p\frac{\lambda_j}{\lambda_j+\lambda}\frac{v^T_jy}{\sqrt{\lambda_j}}\\
=&\sum_{j=1}^p f(\lambda_j)z_j
\end{split}
\end{equation}

\begin{equation}
\hat{y}_\lambda=X\hat{\beta}_\lambda^{RR}=V\Sigma U^TU(\Lambda+\lambda I)^{-1}\Sigma\Lambda y=V\Sigma(\Lambda+\lambda I)^{-1}\Sigma V^T y
\end{equation}

\subsubsection{Choix de mod\`ele ($\lambda$) + ridge path validation crois\'ee}
\begin{itemize}
        \item Ensemble d'apprentissage permet de construire le mod\`ele
        \item Ensemble de test permet d'\'evaluer la qualit\'e du mod\`ele 
\end{itemize}

N\'ecessite de d\'eterminer $\lambda$ sur la base d'un crit\'ere objectif

Un mod\`ele devrait predire efficacement des individus qui n'ont pas servi \`a sa construction

Pour \'evaluer la qualit\'e du mod\`ele, on utilise des strat\'egies de validation crois\'ee.

K-fold cross validation
\begin{enumerate}
\item Partition du jeu de donn\'ees en $K$ parties de taille \'egale $T_1,\ldots,T_K$
\item Pour chaque $k=1,\ldots,K$ construire $\hat{\beta}_\lambda^{-k}$ sans $T_k$
\item $\hat{y}_\lambda^{-k}=T_k\hat{\beta}_\lambda^{-k}$
\item Calcul d'erreurs en test
\begin{equation}
CV_{erreur}(\lambda,k)=\frac{1}{n_k}\sum(y_k-\hat{y}_\lambda^{-k})^2
\end{equation}
Ainsi, Taux d'erreur global($\lambda$)
\begin{equation}
=\frac{1}{K}\sum_{k=1}^KCV_{erreur}(\lambda,k)
\end{equation}
Ainsi,
\begin{equation}
\lambda^*=argmin(Taux d'erreur global(\lambda))
\end{equation}
\end{enumerate}

On appelle que 
\begin{equation}
\hat{\beta}^{RR}=argmin\{||y-x||^2_2+\lambda||\beta||_2^2\}=(X^TX+\lambda I)^{-1}X^Ty
\end{equation}

Proposition
$\forall \lambda>0$ on a 
\begin{equation}
(X^TX+\lambda I_p)^{-1}X^T=X^T(XX^T+\lambda I_\lambda)^{-1}
\end{equation}

Preuve
\begin{equation}
(X^TX+\lambda I_p)^{-1}X^T(XX^T+\lambda I_n)=X^T
\end{equation}
\begin{equation}
(X^TX+\lambda I_p)^{-1}(X^TX+\lambda I_p)X^T=X^T
\end{equation}
Il vient de cette proposition une formulation duale pour $\hat{\beta}^{RR}$
\begin{equation}
\begin{split}
\hat{\beta}^{RR}=&X^T(XX^T+\lambda I_n)^{-1}y\\
=&X^T\hat{\alpha}^{RR}
\end{split}
\end{equation}

Remarque: $\hat{\beta}^{RR}$ s'exprime comme une combinaison lin\'eaire des observations.

En termes de pr\'ediction
\begin{equation}
\hat{y}_{\lambda}=X\hat{\beta}_\lambda^{RR}=XX^T\hat{\alpha}^{RR}
\end{equation}
\begin{equation}
h(x)=\sum_{i=1}^n \alpha_i x_i^Tx
\end{equation}

Conclusion

On remarque que $\hat{\alpha}$ et $\hat{y}$ ne s'expriment qu'au
(Photo)

\section{M\'ethode \`a noyaux}
Introduction Supposons que l'on recherche \`a r\'esoudre un probl\`eme de r\'egression non-lin\'eaire.

Il est tout \`a fait possible d'utiliser une fonction de redescription $\Phi:\mathbb{R}^p\to\mathcal{F}$(espace de description).
La solution du probl\`eme de r\'egression lin\'eaire dans l'espace de redescription est de la forme 
\begin{equation}
\begin{split}
h(x)=&<\beta,\Phi(x)>\\
=&\sum_{j=1}^n\beta_j\Phi_j(x)+\beta_0
\end{split}
\end{equation}
avec $Phi(x)=\{\Phi_1(x),\Phi_2(x),\ldots\}$

En r\'esolvant comme pr\'ec\'edemment dans l'espace de redescription, devient la repr\'esentation duale.
\begin{equation}
h(x)=\sum_{i=1}^n\Phi(x_i)^T\Phi(x)
\end{equation}

En rapprochant $h(x)=\beta^T\Phi(x)$ et $h(x)=\sum_{i=1}^n\alpha_i\Phi(n_i)^T\Phi(x)$
devient 
\begin{equation}
\beta^T=\sum_{i=1}^n\alpha_i\Phi(x_i)^T
\end{equation}

Commentaire 1: Comment trouver une fonction de redescription ad\'equate?

Commentaire 2: Co\^{u}t calculatoire des produits scalaires dans un espace $\mathcal{F}$ dont la dimension peut \^etre tr\`es grande.

Plut\^ot que de repr\'esenter les observations par un tableau individus $x$ variables, on peut les repr\'esenter par une matrice de similarit\'e de dimension $n\times n$
\begin{equation}
[K]_y=k(x_i,x_j)
\end{equation}
\begin{equation}
[X]\sim [K]
\end{equation}
\begin{equation}
(x_i)_{n\times p}\sim (k(x_i,x_n))_{n\times n}
\end{equation}

Remarque: Mesure de similarit\'e quelle que soit la nature et la complexit\'e des objets.(images, graphs, s\'equences ADN,...)
Remarque 2: La matrice $K$ est toujours de dimension $n\times n \forall$ la taille de l'objet.

Dans ce cours nous nous restrienons \`aune classe particuli\`res de fonction $k$.

D\'efinition Fonction semi definie positive

Une fonction $k$ semi definie positive sur l'ensemble $X$ est une fonction $k:X,X\to \mathcal{R}$ symm\'etrique 
\begin{equation}
\forall (x,x')\in X, k(x,x')=k(x',x)
\end{equation}
et quisatisfait pour 
(Photo)

et $(a_1,\ldots,a_n)\in\mathcal{R}^n$
\begin{equation}
\sum_{i,j=1}^na_ia_jk(x_i,k_j)>=0
\end{equation}
En d'autres termes $K=(k(x_i,x_j))$ est semi definie positive.

Exemple 1: Le plus simple des noyaux semi definie positif

Soit $X\in\mathbb{R}^P$ et la fonction $k:\mathcal{X}^2\to\mathbb{R}$ definie par
\begin{equation}
\forall (x,x')\in (\mathbb{R}^p)^2, k(x,x')=<x,x'>_{\mathbb{R}^p}
\end{equation}
Sym\'etrie
\begin{equation}
<x,x'>_{\mathbb{R}^p}=<x',x>_{\mathbb{R}^p}
\end{equation}

Th\'eor\`eme de Moose-Aroszajin, 1950

(Photo)

Definition: Soit $\mathcal{H}$

(Photo)

2. La fonction $k$ est une fonction noyau reproduisante 
\begin{equation}
\forall f\in \mathcal{H} on a <f,k(n,.)>=f(x)
\end{equation}

Le produit scalaire est alors d\'efini comme suit:

Soit $f$ et $g\in\mathcal{H}$ d\'efinies par
\begin{equation}
f(x)=\sum_{i=1}^n\alpha_ik(x_i,x)
\end{equation}
et
\begin{equation}
g(x)=\sum_{i=1}^n\beta_jk(x_j,x)
\end{equation}
alors
\begin{equation}
\begin{split}
<f,g>=&\sum_{j=1}^n\sum_{i=1}^n\alpha_i\beta_jk(x_i,x_j)\\
=&\sum_{i=1}^n\alpha_ig(x_i)\\
=&\sum_{j=1}^n\beta_jf(x_j)
\end{split}
\end{equation}

Representation theorem (Kimeldoif \& Wabhia 1970)

Soit $\mathcal{H}$ un RKHS associ\'e \`a un noyau $k$.
Soit $\Omega [0,+\infty [\to\mathbb{R} $ une fonction strictement croissante
Soit $\mathcal{X}$ un emsemble et $l(\mathcal{X}\times\mathbb{R})\to\mathbb{R}$ une fonction de co\^ut.

Alors toute solution du probl\`ele d'optimisation suivant 
\begin{equation}
\min_fJ_f=l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))+\lambda\Omega(||f||_\mathcal{H})
\end{equation}
admet une repr\'esentation de la forme $f^*=\sum_{i=1}^n\alpha_ik(x_i,.)$

Remarque 1
\begin{equation}
l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))+\lambda\Omega(||f||_\mathcal{H})
\end{equation}
conduit \`a la regression rigide.

Remarque 2:
Quelque soit la dimension de l'espace $\mathcal{H}$, la solution \'evolue dans $span\{k(x_i,.),1<=i<=n\}$
de dimension $n$ connue, bien que $H$ puisse \^etre de dimension imporeante.
$\Rightarrow$ Developpement d'algo thno efficace.

\textbf{Preuve du th\'eoreme du representant}
Supposons $f\in\mathcal{H}$ project\'ee sur $span{k(x_i,.),1<=i<=n}$
Soit $f_s$(la composante dans l'espace engendr\'e) et $f_\perp$ la composante orthogonale
\begin{equation}
f=f_s+f_\perp
\end{equation}
Ainsi
\begin{equation}
||f||_\mathcal{H}^2=||f_s||^2_\mathcal{H}+||f_\perp||_\mathcal{H}^2>=||f||_\mathcal{H}^2
\end{equation}
Puisque $\Omega$ est strictement croissante, on a 
\begin{equation}
\Omega(||f||_\mathcal{H}^2)>\Omega(||f_s||_\mathcal{H}^2)
\end{equation}
Cela signifie que $\Omega$ est minimis\'ee pour des fonctions $f\in span\{k(x_i,.),1<=i<=n\}$

Par ailleurs, des propri\'et\'es reproduisantes de $k$ on obtient
\begin{equation}
f(x_i)=<f,k(x_i,.)>=<f_s,k(x_i,.)>_\mathcal{H}+<f_\perp,k(x_i,.)>_\mathcal{H}=<f_s,k(x_i,.)>=f_s(x_i)
\end{equation}
Par cons\'equent,
\begin{equation}
l((x_1,y_1,f(x_1)),\ldots,(x_n,y_n,f(x_n)))=((x_1,y_1,f_s(x_1)),\ldots,(x_n,y_n,f_s(x_n)))
\end{equation}
Donc $l$ ne d\'epend que des composantes de $f\in span\{k(x_i,.),1<=i<=n\}$ et $\Omega$ est minimis\'ee que si $f\in$ cet espace

Conclusion:
$J(f)$ est minimis\'ee que si $f\in span\{k(x_i,.),1<=i<=n\}$ et ;a solution est fonc de la forme
\begin{equation}
f^*=\sum_{i=1}^n\alpha_ik(x_i,.)
\end{equation}

Kernel Rigide Regression

Un point d'une fonctionnelle

Soit $\mathcal{H}$ un RKHS de noyau $k$. On consid\`ere le probl\`eme suivant
\begin{equation}
f^*=argmin_{f\mathcal{H}}\{\frac{1}{2}\sum_{i=1}^n (y_i-f(x_i))^2+\frac{\lambda}{2}||f||_\mathcal{H}^2\}
\end{equation}
$=$ termed'attache aux donn\'ees $+$ terme de r\'egularisation.

Par le th\'eor\`eme de repr\'esentant, on sait que 
\begin{equation}
f^*\sum_{j=1}^n\alpha_j k(x_j,.)
\end{equation}

(Photo)

Exercise: Montrer que

(Photo)

Kernel Ridige Regression

(Photo)

Franchement, le choix de la fonction $Phi$ se ramene au choix d'une fonction $k$ sdp.

Exemple demonstratif:

similarity:
focntion noyau
\begin{equation}
k(x_i,x_j)=(1+x_i^Tx_j)^2
\end{equation}
On va construire explicitement l'espace induit par la fonction $k$.
\begin{equation}
\begin{split}
k(x_i,x_j)=&1+x_{i1}^2x_{j1}^2+2x_{i1}x_{j1}x_{i2}x_{j2}+x_{i2}^2x_{j2}^2+2x_{i1}x_{j1}x_{i2}x_{j2}\\
=&[1 x_{i1}^2 \sqrt{2}x_{i1}x_{i2} x_{i2}^2 \sqrt{2}x_{i1} \sqrt{2}x_{i2}]^T[1 x_{j1}^2 \sqrt{2}x_{j1}x_{j2} x_{j2}^2 \sqrt{2}x_{j1} \sqrt{2}x_{j2}]\\
=&\Phi(x_i)^T\Phi(x_j)
\end{split}
\end{equation}

Astude du noyau
\begin{equation}
\hat{\alpha}^{RR}=(XX^T+\lambda I_n)^{-1}y
\end{equation}
\begin{equation}
\hat{\alpha}^{RR}=(K+\lambda I_n)^{-1}y
\end{equation}

Autres exemples de noyaux,
Noyau polynomial
\begin{equation}
k(x,y)=(c+x_i^Tx_j)^p
\end{equation}
Noyau gaussien
\begin{equation}
exp(-\frac{||x_i-x_j||^2}{2\sigma^2})
\end{equation}

\section{Analyse discriminante}
Introduction
Soit $Y$ une variable \`a expliquer QUALITATIVE \`a $K$ cat\'egorie $y\in{1,\ldots,K}$

Soient $X_1,\ldots,X_p$ les $p$ variables explicatives.

Objectif 1: L'analyse discriminante
descriptive: Proposer un syst\`eme de repr\'esentation qui permet de discerner le plus possible les differents groupes d'individus.
$Rightarrow$ L'analyse factorielle discriminante

Objective 2: Construire une fonction de classement qui permet de pr\'edire le groupe d'appartenance d'un individu \`a partir des valeurs prises par les variables explicatives.
$Rightarrow$ Analyse Discriminante Bayesienne

Soit $n_j$ le nombre d'observation appartenant au groupe $j$. Soit $n$ le nombre d'observations total.
\begin{equation}
g=(g_1,\ldots,g_p)
\end{equation}
les centres de gravit\'e du nuage global.
noyaux des $X_j$ calcul\'e \`a partir de toute les observations.
\begin{equation}
m_k=(m_{k1},\ldots,m_{kp})
\end{equation}
centre de gravit\'e du nuage des individus de la class $k$.
\begin{equation}
V=\frac{1}{n}\sum_{i=1}^{n}(x_i-y)(x_i-y)^T
\end{equation}
matrice de covariance des donn\'ees compl\`etes.
\begin{equation}
B=\frac{1}{n}\sum_{k=1}^Kn_k(g_k-g)(g_k-g)^T
\end{equation}

\begin{equation}
W-\frac{1}{n}\sum_{k=1}^{K}n_kV_k
\end{equation}
avec
\begin{equation}
V_k=\frac{1}{n_k}\sum_{i \in \mathcal{C}_k}(x_i-m_k)(x_i-m_k)^T
\end{equation}
qui est la matrice de covariance intraclasse.

On peut montrer(exercise) que(voir TP)
\begin{equation}
V=B+W
\end{equation} 

Construisons la variance globale du nuage de points.
\begin{equation}
\begin{split}
\sigma^2=&\frac{1}{n}\sum_{i=1}^n(x_i-y)(x_i-y)^T\\
=&\frac{1}{n}\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}(x_i-g)^*(x_i-g)\\
=&\frac{1}{n}\sum_{k=1}^KSS(k)
\end{split}
\end{equation}
\begin{equation}
\begin{split}
SS(k)=&\sum_{i\in\mathcal{L}_k}(x_i-g)^T(x_i-g)\\
=&\sum_{i\in\mathcal{L}_k}(x_i-m_k+m_k-g)^T(x_i-m_k+m_k-g)\\
=&\sum_{i\in\mathcal{L}_k}(||x_i-m_k||^2+||m_k-g||^2+2(x_i-m_k)(m_k-g))\\
=&\sum_{i\in\mathcal{L}_k}(||x_i-m_k||^2+||m_k-g||^2)
\end{split}
\end{equation}

Donc, la variance totale s'\''ecrit
\begin{equation}
\begin{split}
\sigma^2=&\frac{1}{n}[\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}||x_i-m_k||^2+\sum_{k=1}^Kn_k||m_k-m||^2]\\
=&\frac{1}{n}\sum_{k=1}^Kn_k\frac{1}{n_k}\sum_{i\in\mathcal{L}_k}||x_i-m_k||^2+\sum_{k=1}^K||m_k-g||^2\\
=&\frac{1}{n}\sum_{k=1}^Kn_k[\frac{1}{n_k}\sum_{i\in\mathcal{L}_k}||x_i-m_k||^2+||m_k-g||^2]
\end{split}
\end{equation}
Si l'on definit une operation de projection $\Pi=vv^T$avec $v^Tv=1$
\begin{equation}
\begin{split}
	\sigma_{within}^2=&\frac{1}{n}\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}(\Pi_{x_i}-\Pi_{m_k})^T(\Pi_{x_i}-\Pi_{m_k})\\
	=&\frac{1}{n}\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}(vv^Tx_i-vv^Tm_k)^T(vv^Tx_i-vv^Tm_k)\\
	=&\frac{1}{n}\sum_{k=1}^K(x_i-m_k)^Tvv^Tvv^T(x_i-m_k)
\end{split}
\end{equation}

\begin{equation}
	\begin{split}
		\sigma_w^2=&\frac{1}{n}\sum_{k=1}^K\sum_[i\in\mathcal{L}_k](x_i-m_k)^Tvv^T(x_i-m_k)\\
		=&\frac{1}{n}\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}v^T(x_i-m_k)(x_i-m_k)^Tv\\
		=&v^T[\frac{1}{n}\sum_{k=1}^K\sum_{i\in\mathcal{L}_k}(x_i-m_k)(x_i-m_k)^T]v\\
		=&v^T[\frac{1}{n}\sum_{k=1}^Kn_k\sum_{i\in\mathcal{L}_k}(x_i-m_k)(x_i-m_k)^T]v\\
		=&v^Tv
	\end{split}
\end{equation}

Pour la variance between projet\'ee sur $V$
\begin{equation}
\begin{split}
\sigma^2_{between}(v)=&\frac{1}{n}\sum_{k=1}^Kn_k(\Pi m_k-\Pi y)^T(\Pi m_k-\Pi y)\\
=&v^T[\sum_{k=1}^Kn_k \frac{1}{n}(m_k-y)(m_k-y)^T]v
\end{split}
\end{equation}
Comme $V=B+W$
\begin{equation}
\sigma^2_{total}(v)+v^TVv
\end{equation}
L'analyse discriminante factorielle est  d\'efinie par le probl\`eme d'optimisation suivant
\begin{equation}
\max(\frac{v^TBv}{v^TWv})
\end{equation}
On remarque si $v^*$  est solution alors $\alpha v^*$ est \'egalement solution
\begin{equation}
\max(v^TBv),\text{ sc } v^TWv=1
\end{equation}

Remarque:  On aurait pu  \'egalement consid\'erer le probl\'eme d'optimisation suivant
\begin{equation}
\max  (\frac{v^TBv}{v^TVv})\Leftrightarrow \max v^TBv \text{ sc } v^TVv=1
\end{equation}

Considerons le lagrangien associ\'ee \`a ce probl\`eme d'optimisation
\begin{equation}
L=v^TBv-\lambda (v^TVv-1)
\end{equation}
En annulant la d\'eriv\'ee de $L$ par rapport \`a $v$, devient
\begin{equation}
\frac{\partial L}{\partial v}=2Bv-\lambda Vv=0
\end{equation}
Ainsi,
\begin{equation}
V^{-1}Bv=\lambda v
\end{equation}
La solution est obtenue en consid\'erant $v$ le premier vecteur propre.

Commentaire: Le nombre de vecteurs propres associ\'e \`a des valeurs propres non nulles est \'egal \`a $K-1$ (du fait du rang de $B$)

\subsection{Analyse Discriminante Bayesienne}

On cherche \`a construire une r\`egle de pr\'ediction sur l'affectation  d'un individu \`a l'un des groupes.
On s'int\'eresse \`a estimer
\begin{equation}
p_k(x)=P(y=k|X_1=x_1, \ldots,X_p=x_p)
\end{equation}

On effectuera l'observation un groupe le plus probable.
On utilise la formule de Bayesienne pour calculer $p_k(x)$  avec l'hypoth\`ese que le $X=(X_1,\ldots,X_p)\sim\mathcal{N}(\mu_k,\Sigma_k)$ sur chaque population et en fonction des probabilis\''es a priori $q_k=\mathbb{P}(y=k)$ d'appartenance aux differentes sous-population. La formule de Bayes permet d'obtenir la probabilit\'e a posterori $p_k(x)$ en fonction de la densit\'e $f_k(x)$
 et des probabilit\'e $q_k$..
\begin{equation}
p_k(x)=\mathbb{P}(Y=k|X=x)=\frac{f_h(x)q_h}{\sum_{k=1}^K f_h(x)q_k}
\end{equation}
\subsubsection{Cas de matrice de covariances homog\`ene}
Ici, on suppose que $X=(X_1,\ldots,X_p)\sim\mathcal{N}(\mu_h,\Sigma)$ sur la sous population $\mathcal{P}_k$

(Photo)

En remplacant la moyenne $\mu_h$ par leur estimation,
\begin{equation}
\hat{\mu}_h=m_h
\end{equation}
\begin{equation}
\hat{q}_k=\frac{n_k}{n}
\end{equation}
Et la matrice de covariance par son estimation
\begin{equation}
S=\frac{1}{n-k}\sum_{k=1}^Kn_kV_k
\end{equation}
On obtient des fonctions discriminantes lin\'eaire $d_h(x)$ suivante
\begin{equation}
d_h(x)=\frac{1}{2}m_k^TS^{-1}m_k-h^TS^{-1}x+\ln(\hat{q}_h)
\end{equation}
on en d\'eduit une estimation des probabilit\'es a posteriori
\begin{equation}
\hat{p}_h(x)=\frac{exp(d_h(x))}{\sum_{k=1}^K exp(d_k(x))}
\end{equation}
Un nouvel individu sur lequel a \'et\'e observ\'e les valeurs $x=(x_1,\ldots,x_p)$ est affect\'e au groupe $h$ pour lequel $d_h(x)$ est maximum.

\subsubsection{Cas o\`u les covariances sont h\'et\'erog\`ene}
On suppose maintenant que $X=(X_1,\ldots,X_p)$ est une loi multinomial $\mathcal{N}(\mu_h,\Sigma_h)$ sur chaque sous population $\mathcal{P}_h$.
Avec un raisonnement analogue et enconsid\'erant
\begin{equation}
\hat{\mu}
\end{equation}

On obtient des fonctions discriminantes quadratiques definie par 
\begin{equation}
d_h(x)=-\frac{1}{2}(x-m_k)^TS_h^{-1}(x-m_h)
\end{equation}

(Photo)

Pour chacun entre la version lin\'eaire ou quadratique, on proc\`ede par cross validation.

\section{Les Support Vector Machines(SVM)}
\textbf{Introduction}
Dans ce cours, on se concentre sur des problematiques de classification binaire. L'approche propos\'ee initialement par (Boser et al.  1995) repose sur la notion intuitive d'hyperplan s\'eparable de marge maximale. Ce nouveau concept est motiv\'e par la question suivante: 

Comment construire \`a partir d'une base d''apprentissage finie une fonction de d\'ecision capable de pr\'edire efficacement la classe d'appartenance d'un individu qui n'a pas servi \`a construire le mod\`ele.

\subsection{SVM Lin\'eaire et classe lin\'eaire s\'eparable}

Soient $n$ observations d\'2crites par $p$ variables s\'eparable en $2$ classes.
\`A chaque observation $x_i$ est donc associ\'e un label $y\in\{-1,1\}$.

Soit un hyperplan d\'efini par
\begin{equation}
\mathcal{H}=\{x|f(x)=x^T\beta+\beta_0=0\}
\end{equation}

Si les donn\'ees sont lin\'eairement s\'eparable, alors. Il existe une infinit\'e d'hyperplan tel que 
\begin{equation}
y_if(x_i)>0
\end{equation}

L'hyperplan  s\''eparateur optimal  est d\'efini par les vecteurs $\beta$ et $\beta_0$ comme suit:

Trouver $\beta$ et $\beta_0$ qui rendent maximal la distance minimale des individus \``a l'hyperplan.

La distance d'un individu de $\mathcal{C}_1$ \`a l'hyperplan $H$
\begin{equation}
d(x,\mathcal{H})=x^T\beta-(-\beta_0)=x^T\beta+\beta_0
\end{equation}

La distance d'un individu de $\mathcal{C}_{-1}$  \`a l'hyperplan $\mathcal{H}$
\begin{equation}
d(x,\mathcal{H})=-\beta_0-x^T\beta
\end{equation}
Ainsi, $x^T\beta+\beta_0$ repr\'esente  la distance "sign\'ee" de $x$ \`a l'hyperplan $\mathcal{H}$. Cette distance est positive si $x\in\mathcal{C}_1$ et n\'egative si  $x\in\mathcal{C}_{-1}$. De plus, $y_if(x_i)>=0,\forall i$. Si les donn\'ees sont lin\'eairement s\'eparables.

Conclusion

L'hyperplan s\'eparateur  optimal maximise les distances sign\'ees entre les points et l'hyperplan $\mathcal{H}$.
On cherche donc \`a maximiser l'\'epaisseur (de la marge par rapport \`a $\beta$ et $\beta_0$).
\begin{equation}
||\beta||=1
\end{equation}

Si $x_i$ \`a droite de 
(Photo)

Ainsi, maximiser $C$ servient \`a minimiser $||\beta||_2$(lien avec la ridge)
Par cons\'equent, dans le cas de $2$ classes lin\'eairement s\'eparables. La recherche de l'hyperplan s\'eparateur optimal servient \`a r\'esoudre le probl\`eme d'optimisation quadratique suivant: 
\begin{equation}
\min_{\beta,\beta_0}\frac{1}{2}||\beta||^ \text{ sc } y_i(x_i^T\beta+\beta_0)>=1, \forall i=1,\ldots,n
\end{equation}
Les contraintes d\;e finissent une marge de par et d'autre de l'HSO les valeurs $x_i$ tel que 
\begin{equation}
y_i(x_i^T\beta+\beta_0)==1
\end{equation}
Sont situ\'e sur la marge et sont appel\'es "support vectors".

Il faut maintenant r\'esoudre ce probl\`eme d'optimisation. 

D\'efinissons le Largrangien associ\'e 
\begin{equation}
L(\beta,\beta_0,\alpha)=\frac{1}{2}||\beta||^2\sum_{i=1}^n \alpha[y_i(x_i^T\beta+\beta_0)-1]
\end{equation}

o\`u les $x_i>=0$ sont les multiplicateurs de Lagrange.

On duit minimiser par rapport \`a  $\beta$ et $\beta_0$ et maximiser par rapport \`a $\alpha$. En annulant la d\'eriv\'ee du Lagrangien par rapport \`a $\beta$ et $\beta_0$, on obtient
\begin{equation}
\frac{\partial L}{\partial \beta}-\beta-\sum_{i=1}^^n\alpha_i y_ix_i
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta}=-\sum_{i=1}^n\alpha_i y
\end{equation}

En d\'evceloppant cette formule et injecter () et (), on atteint la formulation duale du  Lagrangien 
\begin{equation}
L(\alpha)=\sum_{i=1}^n\alpha_i-\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j
\end{equation}
qui ne d\'epend pas de $\beta$ et $\beta_0$. On cherche ensuite \`a maximiser $L(\alpha)$ par rapport \`a $\alpha$ sous les contrainte
\begin{equation}
\left\{\begin{array}{ll}
\alpha_i>=0 &  i=1,\ldots,n\\\
\sum_{i=1}^n\alpha_iy_i=0
\end{array}\right.
\end{equation}

C'est un probl\`eme d'optimisation quadratique sous contraintes lin\'eaire.
Ainsi $quadprog/ipop$.

Soit $\alpha^*$ la solution. La condition $KKT$ devient ici
\begin{equation}
\alpha_i^*[y_i(x_i^T\beta_0^*+\beta_0^*)-1]=0
\end{equation}
Si $\alpha_2^*>=0$,
\begin{equation}
\Rightarrow y_i(x_i^T\beta^*+\beta_0^*)=1
\end{equation}
$\Rightarrow$ la contrainte est satur\'ee
$\Rightarrow$ $x_i$  est sur la marge et est un vecteur de support.

Posons
\begin{equation}
S=\{i|\alpha_i^*>0\}
\end{equation}
L'ensemble des $i$ associ\'e aux vecteurs de support. Inversement, si $x_i$ n'est pas un vecteur de support alors,
\begin{equation}
y_i(x_i^T\beta++\beta_0)>1
\end{equation}
et donc $\alpha_i^*=0$.

Une fois r\'esolu le probl\`eme dual, on en d\'eduit la solution $(\beta^*,\beta^*_0)$ du probl\`eme initial.

En effet, on a
\begin{equation}
\beta^*=\sum_{i=1}^n\alpha_i^* y_i x_i=\sum_{i\in S}\alpha_i^* y_i x_i
\end{equation}

Par ailleurs, pour tout  $i\in S$,
\begin{equation}
y_i(x_i^T\beta^*+\beta^0)=1\Rightarrow \beta_0^*=\frac{1}{y_i}-x_i^T\beta^*=y_i-x_i^T\beta^*
\end{equation}

Remarque importante

La solution du probl\`eme initial s'exprime donc uniquement en fonction des support vectors(SV).

Conclusion
L'HSO a donc pour \`equation
\begin{equation}
\begin{split}
	f(x)=&x^T\beta^*+\beta^*_0\\
=&\sum_{i=1}^n\alpha_i^* y_i x_i^Tx+\beta_0^*\\
=&\sum_{i\in S}\alpha_i^*y_ix_i^T x+\beta_0^*
\end{split}
\end{equation}

La fonction de d\'ecision correspondante est 
\begin{equation}
d(x)=sign(f^*(x))
\end{equation}

Remarque fondamentale

La fonction de d\'ecision et le probl\`eme d'optimisation ne s'exprime qu'qu travers de produits scalaires entre observations

\subsection{SVM Lin\'eaire cas non-lin\'eairement s\'eparable}

Dans le cas non lin\'eairement s\'eparable, l'HSO doit \'etablir un compromis entre largeur de marge "raisonnable" et taux d'erreur. 
La solution consiste \`a autoriser certains individus \`a avoir une marge plus petite que $1$ voire n\'egative. Les contraintes doivent \^etre retranch\'es de facon douce en introduisant des variables ressorts $\\epsilon_i,i=1,\ldots,n$. On va remplacer 
\begin{equation}
y_i(x_i^T\beta+\beta_0)>1
\end{equation}
par
\begin{equation}
y_i(x_i^T\beta++\beta_0)>1-\epsilon_i
\end{equation}

On d\'efinit donc le probl\`eme d'optimisation suivant
\begin{equation}
\min \frac{1}{2}||\beta||^2+\\gamma\sum_{i=1}^n\beta_i
\end{equation}
sc
\begin{equation}
\left\{\begin{array}{rcl}
y_i(x_i^T\beta+\beta_0)>=1-\epsilon_i & i=1,\ldots,n\
\epsilon_i>=0 & i=1,\ldots,n
\end{array}\right.
\end{equation}

Remarque 1: Le param\`etre $\gamma$ est un param\`etre de r\'egularisationpermettant d'\'etablir un compromis entre dargeur de marge "raisonnable" et taux d'erreur.

Remarque 2:
\begin{equation}
y_i(x_i^T\beta++\beta_0)>=1-\epsilon_i\Leftrightarrow \epsilon_i>=1-y_i(x_i^T\beta+\beta_0)
\end{equation}
Dans ce cas, le Largrangian s'ecrit
\begin{equation}
\begin{split}
L(\beta,\beta_0,\epsilon,\alpha,\mu)=&\frac{1}{2}||\beta||^2+\gamma\sum_{i=1}^n\epsilon_i-\sum_{i=1}^n\alpha_i(y(x_i^T\beta+\beta_0)-(1-\epsilon_i))-\sum_{i=1}^n\mu_i\epsilon_i
\end{split}
\end{equation}

En annulant la d\'eriv\'ee du Largrangian par rapport aux $\beta,\beta_0$ et $\epsilon_i$
\begin{equation}
\frac{\partial L}{\partial \beta}=\beta-\sum_{i=1}^n\alpha_iy_ix_i=0
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta_0}=-\sum_{i=1}^n x_iy_i=0
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \epsilon_i}=\gamma-\alpha_i-\mu_i=0,i=0,\ldots,n
\end{equation}

En d\'eveloppant cette formule et en injectant les constantes, on obtient 
\begin{equation}
L(\alpha)=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_ix^T_ix_j
\end{equation}
m\^eme expression que dans le cas lin\'eairement s\'eparabl.

Le probl\`eme d'optimisation g\'en\'eral des SVM consiste \`a maximiser ()  sous les contraintes:
\begin{equation}
\left\{\begin{array}{lcr}
\alpha_i>=0 & i=1,\ldots,n\\
\mu_i>=0 & i=1,\ldots,n\\
\mu_i=\gamma-\alpha_i & i=1,\ldots,n\\
\sum_{i=1}^n\alpha_iy_i=0
\end{array}\right.
\Leftrightarrow\left\{\begin{array}{l}
0<=\alpha_i<=\gamma\\
\sum_{i=1}^n\alpha_iy_i=0
\end{array}\right.
\end{equation}

En r\''esum\'e, SVM est d\'efini par le probl\`eme d'optimisation suivant
\begin{equation}
whatever
\end{equation}

Soit $\alpha^*$
 la solution de (),  les conditions KKT se ram\`ene ici KTT1
 \begin{equation}
 \alpha_i^*[y_i(x_i^T\beta^*+\beta^*_0)-(1-\epsilon_i)]=0
\end{equation}
KKT 2:
\begin{equation}
\mu_i^*\epsilon_i^*=0
\end{equation}

Les SV sorrespondent aux $x_i$ associ\'es \`a des $\alpha_i^*>0$.. Ils  v\''erifient
\begin{equation}
y_i(x_i^T\beta^*+\beta^*_0)=1-\epsilon_i^*
\end{equation}
Les SV sont soit les $x_i$
 mal class\'es $(1-\epsilon_i)<0$, soit les $x_i$ \`a l'int\'erieur de la marge.
 
 Si de plus $\mu_i^*>0$ ($0<\alpha_i<Y$), alors,, $\epsilon_i=0$ et dans ce cas, les SV $x_i$ sont sur la marge.
 
 Posons $S=\{i|\alpha_i^*>0\}$ l'ensemble des indices associ\'ees  aux SV.
 
 Une fois resolu le probl\`eme dual
\begin{equation}
\beta^*=\sum_{i=1}^n\alpha_i^*y_ix_i=\sum_{i\in S}\alpha_i^*y_ix_i
\end{equation}

Par ailleurs, pour obtenir $\beta_0^*$, il suffit de consid\'erer un SV sur la marge ($0<\alpha<\gamma$) puis d'utiliser KKT 1
\begin{equation}
\beta_o^*=\frac{1}{y_i}-x_i^T\beta^*=y_i-x_i^T\beta^*
\end{equation}
La fonction de d\'ecision est la  m\^eme que pr\'ec\'edamment
\begin{equation}
f^*(x)=\sum_{i\in S}\alpha^*y_ix_i^Tx+\beta_0^*
\end{equation}
$\\Rightarrow$
\begin{equation}
d^*(u)=sign(f^*(x))
\end{equation}
\subsection{SVM comme une probl\'ematique de fonction de co\^ut p\'enalis\'e}
\begin{equation}
\min_\beta\frac{1}{2}||\beta||^2++\gamma\sum_{i=1}^n
\end{equation}
sc
\begin{equation}
y_i(x_i^T\beta+\beta_0)>=1-\epsilon_i, i=1,\ldots,n
\end{equation}
et
\begin{equation}
\epsilon_i>=0,  i=1,\ldots,n
\end{equation}
$\Leftrightarrow$
\begin{equation}
\min_\beta \frac{1}{2}||\beta||^2+\gamma\sum_{i=1}^n\epsilon_i
\end{equation}
sc
\begin{equation}
\epsilon_i>=1-y_i(x_i^T\beta+\beta_0)
\end{equation}
\begin{equation}
\epsilon_i>=0
\end{equation}
$\Leftrightarrow$
\begin{equation}
\min_\beta\frac{1}{2}+\gamma\sum_{i=1}^nmax(0,1-y_i(x_i^T\beta+\beta_0))
\end{equation}
$\Leftrightarrow$
\begin{equation}
\min_\beta\sum_{i=1}^n\max(0,1-y_i(x_i^T\beta+\beta_0))+\lambda||\beta||^2
\end{equation}
Premier terme Hinge Loss

Si l'on souhaite \'etendre les SVM lin\'eaire \`a non-lin\'eaire, il suffit d'appliquer le kernel trick.

Les probl\`eme d'optimisation associ\'e au SVM non-lin\'eaire est le suivant
\begin{equation}
\max_\alpha{\sum_{i=1}}
\end{equation}

\section{R\'egression parcimonieuse}
\textbf{Introduction}

Les mod\`eles "parcimonieux" poursuivent deux objectifs
1. Le mod\`ele doit \^etre pr\'edictif
2. La "forme" du mod\`ele doit sugg\'egrer quelles variables sont importantes pour pr\'edire efficacement  la r\'epnse.

Ici, nous pr\''esentons des m\'ethodes attractives \`a la r\'egression ridge, fond\'ee sur des p\'enalit\'es de type $L1$.

Soient $X_1,\ldots,X_p$ un ensemble de variables explicatives  et $y$ vecteur de r\'eponse.
\begin{equation}
\hat{y}=\hat{\beta}_1X_1+\cdots+\hat{\beta}_pX_p
\end{equation}
Dans toute la suite, on va supposer que $X$ est standardis\'ee et $y$ centr\'ee.

\subsection{LASSO Least Absolute Shrinkage and Selection Operation}
La r\'egression LASSO a \'et\'e propos\'e en 1996 par Tib... L'estimation LASSO est d\'efinie par le probl\`eme d'optimisation
\begin{equation}
\hat{\beta}^{LASSO}=argmin_\beta\{||y-X\beta||_2^2+\lambda ||\beta||_1\}
\end{equation} 
Pour comprendre pourquoi la norme $l_1$ conduit \`a des mod\`eles parcimonieux(sparse), pla\c{c}ons nous  dans un cadre simplifi\'e o\`u $X^TX=I$
\begin{equation}
\hat{\beta}^{LASSO}=argmin_\beta\{y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta++\lambda\sum_{j=1}^P|\beta_j|\}=argmin\{L(\beta)\}
\end{equation}
Comme $X^TX=I, L=\cdots$
D\'erivons $L$ par rapport \`a $\beta_j$
\begin{equation}
\frac{\partial  L}{\partial \beta_j}=-2X_j^Ty+2\hat{\beta}_j^{LASSO}+\lambda sign(\hat{\beta}_j^{LASSO})
\end{equation} 
Remarquons
\begin{equation}
\hat{\beta}^{OLS}=(X^TX)^{-1}X^Ty=X^Ty
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta_j}=-2\hat{\beta}_j^{OLS}+2\hat{\beta}_j^{LASSO}+\lambda sign(\hat{\beta}_j^{LASSO})=0
\end{equation}

3 cas de figure se pr\'esentent
\begin{equation}
\hat{\beta}_j^{LASSO}=0\Rightarrow\hat{\beta}_j^{LASSO}=0
\end{equation}
\begin{equation}
\hat{\beta}_j^{LASSO}>0\Rightarrow\hat{\beta}_j^{LASSO}=\hat{\beta}_j^{OLS}-\frac{\lambda}{2}
\end{equation}
\begin{equation}
\hat{\beta}_j^{LASSO}<0\Rightarrow \hat{\beta}_j^{LASSO}=\hat{\beta}_j^{OLS}+\frac{\lambda}{2}
\end{equation}
\begin{equation}
sign(\hat{\beta}_j^{LASSO})=sign(\hat{\beta}_j^{OLS})
\end{equation}

Puisque ces 3 cas sont exclusifs
\begin{equation}
\begin{split}
	\hat{\beta}_j^{LASSO}=&(\hat{\beta}_j^{OLS}-\frac{\lambda}{2})_{\mathbbm{1}_{\hat{\beta}_j^{OLS}-\frac{\lambda}{2}>0\}}}+(\hat{\beta}_j^{OLS}+\frac{\lambda}{2})_{\mathbbm{1}_{\{\hat{\beta}_j^{OLS}+\frac{\lambda}{2}<0\}}}\\
=&(\hat{\beta}_j^{OLS}-sign(\hat{\beta}_j^{OLS}))_{\mathbbm{1}_{\{|\hat{\beta}_j^{OLS}|>\frac{\lambda}{2}\}}}\\
=&sign(\hat{\beta}_j^{OLS})(|\hat{\beta}_j^{OLS}|-\frac{\lambda}{2})_{\mathbbm{1}_{\{|\hat{\beta}_j^{OLS}|-\frac{\lambda}{2}>0\}}}
\end{split}
\end{equation}
\begin{equation}
\hat{\beta}_j^{LASSO}=sign(\hat{\beta}_j^{OLS})\times max(0,|\hat{\beta}_j^{OLS}|-\frac{\lambda}{2})
\end{equation}

Cet op\'eration porte le nom de seuillage doux.

Remarque, Si un groupe de variables est tr\`es corr\'el\'ees, le LASSO aura tendance \`a selectionner une variable dans le groupe.
Pour contributeur ce point Zou \& Hastie en 2005 ont propos\'e Elastic Net f\'efinie par le probl\`eme d'optimisation suivant:
\begin{equation}
\hat{\beta}^{EN}=argmin_\beta\{||y-X\beta||_2^2+\lambda_1||\beta||_2+\lambda_2||\beta||_2^2\}
\end{equation}
\begin{equation}
L=y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta+\lambda_2\beta^T\beta+\lambda_1\sum_{j=1}^P|\beta_j|
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta_j}=-2\hat{\beta}_j^{OLS}+2\hat{\beta}_j^{EN}(1+\lambda_2)+\lambda_1 sign(\hat{\beta}_j^{EN})=0
\end{equation}

3 cas de figure se pr\'esentent
\begin{equation}
\hat{\beta}_j^{EN} = 0 \Rightarrow \hat{\beta}_j^{EN}=0
\end{equation}
\begin{equation}
\hat{\beta}_j^{EN}>0\Rightarrow=\frac{1}{1+\lambda_2}(\hat{\beta}_j^{OLS}-\frac{\lambda_1}{2})
\end{equation}
\begin{equation}
\hat{\beta}_j^{EN}<0\Rightarrow \hat{\beta}_j^{EN}=\frac{1}{1+\lambda_2}(\hat{\beta}_j^{OLS}+\frac{\lambda_1}{2})
\end{equation}
Ainsi
\begin{equation}
\hat{\beta}_j^{EN}=\frac{max(0,|\hat{\beta}_j^{OLS}|-\frac{\lambda_1}{2})}{1+\lambda_2} sign(\hat{\beta}_j^{OLS})
\end{equation}
Commentaires
\begin{equation}
\hat{\beta}^{RR}=argmin_\beta\{||y-X\beta||_2^2+\lambda||\beta||_2^2\}
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta}=-2X^Ty+2\hat{\beta}^{RR}(1+\lambda)=0\\
=-2\hat{\beta}^{OLS}+2\hat{\beta}^{RR}(1+\lambda)=0
\end{equation}
Ainsi
\begin{equation}
\hat{\beta}^{RR}=\frac{1}{\lambda+1}\hat{\beta}^{OLS}
\end{equation}

Plusieurs outils algorithmique pour r\'esoudre le probl\`eme du LASSO Citons les 4 plus couramment utilis\'es.
1. FISTA
2. LARS
3. Gradient Descent
\begin{equation}
\hat{\beta}^{LASSO}=argmin \{||y-X\beta||_2^2+\lambda||\beta||_1\}=f(\beta_1,\ldots,\beta_p)
\end{equation}
Un algorithme de type Gradient Descent a pour principe de base une optimisation altern\'ee.
$\Rightarrow$ Optimiser par rapport \`a $\beta_j$ en fixant tous les autres.
\begin{equation}
\begin{split}
\hat{\beta}_k=argmin_{\beta_k}||y-\sum_{j=1,j\neq k}^P\beta_j X_j-\beta_kX_k||_2^2+\lambda||\beta||_2\\
=&argmin_{\beta_k}||\hat{y}-\beta_kX_k||_2^2+\lambda|\beta_k|
\end{split}
\end{equation}

En supposant les donn\'ees normalis\'es (chacune des colonne de norme \'egale \`a 1)
\begin{equation}
L=\tilde{y}^Ty-2\beta_k^TX^T_ky+\beta_k^T\beta_k+\lambda|\beta_k|
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \beta_k}=-2X_k^T\tilde{y}+2\hat{\beta}_k^{LASSO}+\lambda sign(\hat{\beta}_k^{LASSO})=0
\end{equation}
Avec la m\^eme raisonnement que p\'rc\'edamment,
\begin{equation}
\hat{\beta}_k^{s+1}=sign (X_k^T\tilde{y})max(0,|X^T_k\tilde{y}|-\frac{\lambda}{2})
\end{equation}
En r\'esum\'e, regularisation \& Shrinkage factor
Regression ridge
Regression sur Composant Principal

M\'ethode \`a noyau
\begin{equation}
\min \sum_{i=1}^nl(y_i,f(x_i))+\lambda ||f||_H\to^{KW} \sum_{i=1}^n \alpha k(x_i)
\end{equation}

Analyse Discriminante
Regression Elastic net


\end{document}